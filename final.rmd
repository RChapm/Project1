---
title: "Economics 144: Project 1"
subtitle: "Ryan Chapman and David Su"
output: html_document
---

## I. Introduction

  The data for this project pertains to the labor force participation rate in the United States. The term "labor force participation rate" refers to the percentage of people who are considered part of the labor force out of the total population. In this data, the total population refers to civilians (those not encarcerated or in service) who are 25 years or older with at least a Bachelor's Degree. To be considered part of the "labor force" a person is either searching for employment or actively employed. The data for this project is taken directly from the Federal Reserve Economic Data (FRED) website $^{[1]}$. The data set was compiled from the Current Population Survey and is taken at monthly intervals from January 1992 to December 2019.

  Labor force participation is an important metric for understanding the true state of unemployment in our economy. When unemployment numbers fall, this indicates a decrease in the number of people actively searching for jobs. But, those people no longer labeled "unemployed" might not have found employment, but rather might have stopped the search for employment altogether. This is why it is important to look at figures such as the labor force participation rate to determine if we have a declining labor force. Furthermore, we look here at the subset of the population 25 years and older and with a bachelors degree or higher intentionally. If the labor force participation decreases for those with a degree, this indicates that a decline in the labor force is not a result of simply untrained workers not being qualified for more technical jobs, but rather a potential true net loss of available jobs in the economy. Keeping all of this information in mind, we will now proceed to evaluate the exact nature of the movement in our labor force participation rate.

## II. Results

### 1. Modeling and Forecasting Trend

**(a) Show a time-series plot of your data.**
```{r message=FALSE}
#importing relevant libraries
library(ggplot2)
library(forecast)
library(dynlm)
library(fpp2)
library(MASS)
library(stats)
library(magrittr)
library(tseries)
#reading in the csv data file for labor force participation rate
L=read.csv('LaborForce.csv',header=T)
#creating a time series from the variable LNU01327662 with monthly frequency
Labor = ts(L$LNU01327662,start=1992,frequency=12)
#plotting the data
autoplot(Labor,main="US Labor Force Participation Rate\n(Civilian, 25 or Older, Bachelors or Higher)",ylab="Participation, (%)",xlab="Year")
```

**(b) Does your plot in (a) suggest that the data are covariance stationary? Explain your answer**

The plot does not suggest that the data are covariance stationary. The mean of the data shows a clear decrease over time and this violates the requirements for both weak and strong stationarity.

**(c) Plot and discuss the ACF and PACF of your data.**
```{r}
#computing the auto correlation and partial auto correlation functions of the data
par(mfrow=c(1,2))
acf(Labor)
pacf(Labor)
```

The ACF suggests that the different time intervals show very high correlation which decreases very slowly across greater spans of time. The PACF suggests that the direct correlation (without intermediate time steps) between time intervals is generally statistically insignificant aside from time lags of 1 month and 13 months, with slight indications of correlation at 4, 10, and 26 months.

**(d) Fit a linear and nonlinear (e.g., polynomial, exponential, quadratic + periodic, etc.) model to your series. In one window, show both figures of the original times series plot with the respective fit.**

Below we will experiment with 8 separate models and then choose one linear and one nonlinear model to move forward with.
```{r}
#creating a time variable
t = time(Labor)
#   MODEL 1: Linear Model
model1=dynlm(Labor~t)
plot1=ts(predict(model1),start=1992,frequency=12)
#   MODEL 2: Linear + Periodic Model with 13 month cycle
model2=dynlm(Labor~t+sin(2*pi*t/13)+cos(2*pi*t/13))
plot2=ts(predict(model2),start=1992,frequency=12)
#   MODEL 3: Linear + Periodic Model with cyclical behavior changing over time
model3=dynlm(Labor~t+I(t*cos(2*pi*t/15))+I(t*sin(2*pi*t/15)))
plot3=ts(predict(model3),start=1992,frequency=12)
#   MODEL 4: Linear + Periodic Model with 13 month cycle + an interaction between time and the cyclical behavior for cosine only
model4=dynlm(Labor~t+sin(2*pi*t/13)+t*cos(2*pi*t/13))
plot4=ts(predict(model4),start=1992,frequency=12)
#   MODEL 5: Linear + Periodic Model with 14 month cycle + an interaction between time and the cyclical behavior for cosine only
model5=dynlm(Labor~t+sin(2*pi*t/14)+t*cos(2*pi*t/14))
plot5=ts(predict(model5),start=1992,frequency=12)
#   MODEL 6: Linear + Periodic Model with 12 month cycle
model6=dynlm(Labor~t+cos(2*pi*t/12))
plot6=ts(predict(model6),start=1992,frequency=12)
#   MODEL 7: Linear + Periodic Model with 15 month cycle + an interaction between time and the cyclical behavior
model7=dynlm(Labor~t+t*cos(2*pi*t/15)+t*sin(2*pi*t/15))
plot7=ts(predict(model7),start=1992,frequency=12)
#   MODEL 8: Quadratic + Periodic Model with 13 month cycle + an interaction between time and the cyclical behavior for cosine only
model8=dynlm(Labor~I(t*t)+t*cos(2*pi*t/13)+sin(2*pi*t/13))
plot8=ts(predict(model8),start=1992,frequency=12)
# Using the AIC and BIC to determine which models we will investigate further.
AIC(model1,model2,model3,model4,model5,model6,model7,model8)
BIC(model1,model2,model3,model4,model5,model6,model7,model8)
#plotting the original data, model 1, and model 8
autoplot(Labor,series="Data",main="US Labor Force Participation Rate (Data + Fitted Models)", ylab="Participation, (%)", xlab="Year") + autolayer(plot1, series="Linear") + autolayer(plot8, series="Quadratic+Periodic") + scale_color_manual(values = c("gray", "blue","red"), breaks = c("Data", "Linear", "Quadratic+Periodic"))
```

In order to determine the optimal period length for each of the aforementioned 8 models, we experimented with different period lengths between 12 and 16 years until we found a value that minimized AIC and BIC for that particular model. We also experimented with the nature of the interaction between t and sine and cosine. After creating multiple iterations of each format, we removed many with statistically insignificant variables, very predictable residuals, and high AIC and BIC values. For the remainder of part I, we will use our most basic linear model (`model1`) and our most accurate quadratic and periodic model (`model8`).

**(e) For each model, plot the respective residuals vs. fitted values and discuss your observations.**
```{r}
#plotting the fitted values vs residuals for model1, along with lowess smoother
plot(model1$fit,model1$res,col="skyblue3",pch=20,xlab="Fitted Values",ylab="Residuals",main="Residuals vs. Fitted Values for Model 1",cex=.6,ylim=c(-2,2))
# Plotting Y(Observed) = Y(Predicted) line
abline(h=0,lwd=1.5,col="red")
# plot the lowess smoother curve on top of the residuals
lines(lowess(model1$fit,model1$res),lwd=2)
# create a legend for the lowess smoother
legend(78,2, c(expression(y[obs]==y[pred]), "Lowess Smoother"), fill =c("red", "black"),cex=1,bty="y")
#plotting the fitted values vs resiudals for model 8, along with lowess smoother
plot(model8$fit,model8$res,col="skyblue3",pch=20,xlab="Fitted Values",ylab="Residuals",main="Residuals vs. Fitted Values for Model 8",cex=.6,ylim=c(-2,2))
# Plotting Y(Observed) = Y(Predicted) line
abline(h=0,lwd=1.5,col="red")
# plot the lowess smoother curve on top of the residuals
lines(lowess(model8$fit,model8$res),lwd=2)
# create a legend for the lowess smoother
legend(78,2, c(expression(y[obs]==y[pred]), "Lowess Smoother"), fill =c("red", "black"),cex=1,bty="y")
```

For `model1`, it is clear that the fit of the model overpredicts for high and low values of labor force participation (as indicated by the lowess smoother dipping below 0). It is also clear that the model underpredicts for most values between 76 and 78. The residuals clearly show a non random pattern which is cyclical in its nature and so our model is likely missing much of the true behavior of the data. When compared to the plot for `model8`, we see that the residuals are largely centered about 0 for all predicted values. This indicates that the inclusion of the periodic and/or quadratic components has given us much better predictive accuracy on our data set.


**(f) For each model, plot a histogram of the residuals and discuss your observations.**
```{r}
#plotting histogram of resiudals for model 1, along with normal distribution and density lines
truehist(model1$res,col="skyblue3",xlab="Residuals",ylab="Fraction",main="Histogram of Residuals")
xr=rnorm(800000,mean(model1$res),sd(model1$res))
lines(density(xr),col="black",lwd=2)
lines(density(model1$res),col="red",lwd=2)
legend(3.5,0.04,c("Density","Normal Distr."),fill=c("red","black"),bty='n')
#plotting histogram of resiudals for model 8, along with normal distribution and density lines
truehist(model8$res,col="skyblue3",xlab="Residuals",ylab="Fraction",main="Histogram of Residuals")
xr=rnorm(800000,mean(model8$res),sd(model8$res))
lines(density(xr),col="black",lwd=2)
lines(density(model8$res),col="red",lwd=2)
legend(3.5,0.04,c("Density","Normal Distr."),fill=c("red","black"),bty='n')
```

The residuals for both models appear close to normal distributions peaking respectively at 0 and 0.2. There appears to be many under predictions of small magnitude and a few over predictions of large magnitude for `model8`. This is likely a result of the seasonal behavior of the data which appears to exhibit short periods far below the trend line followed by longer periods just above the trend line. `model1` appears to be largely symmetric in its distribution of errors.

**(g) For each model, discuss the associated diagnostic statistics ($R^2$, $t$−distribution, $F$−distribution, etc.)**
```{r}
#computing summary statistics for our two chosen modles
summary(model1)
summary(model8)
```

$R$-Squared: The adjusted R-squared of `model1` is .9535 while the adjusted R-squared of `model8` is .9729. This indicates that both models offer strong predictive accuracy for the data but `model8` is superior.

$t$-distribution: The absolute value of the $t$-values for both regressions' coefficients are all greater than 2, indicating they offer statistical significance.

$F$-distribution and $p$-Value: The $F$-statistics and $p$-values support our findings from the t-values. The F-statistics for `model1` and `model8` are respectively 6876 and 2409, both of which imply the model offers significant explanatory behavior. The $p$-values for the coefficients in each model all suggest that the coefficients are significantly different from 0.
The $p$-value for both models as a whole are below 2.2e-16 which indicates again that the models offer significant explanatory ability. 
Therefore, both models are clearly statistically significant even though `model8` accounts for far more behavior within the data.


**(h) Select a trend model using AIC and one using BIC (show the values obtained from each criterion). Do the selected models agree?**
```{r}
#Again, computing the AIC and BIC for these two models.
AIC(model1,model8)
BIC(model1,model8)
```

The AIC selects `model8` and the BIC selects `model8` as it appears this model minimizes error even with a higher penalty for more explanatory variables. Due to the lower value of AIC and BIC for `model8` this is the model we will use for the remainder of the project.

**(i) Use your preferred model to forecast h-steps (at least 16) ahead. Your forecast should include the respective uncertainty prediction interval. Depending on your data, h will be in days, months, years, etc.**

We will forecast 5 years ahead (h=60) with prediction intervals of 80% and 95%.
```{r warning=FALSE}
#forecasting the Labor Force Participation rate in 2022 (24 periods ahead)
#creating a time series model so that we can use the forecast function
model8ts=tslm(Labor~I(t*t)+t*cos(2*pi*t/13)+sin(2*pi*t/13))
#Creating the time of the forecast window
newt=seq(2020,2025,1/12)
#Creating the dataset to be forecast upon
newdata=data.frame(newt,cos(2*pi*newt/13),sin(2*pi*newt/13))
names(newdata)=c('t','cos(2*pi*t/13)','sin(2*pi*t/13)')
#creating a forecast object
cast=forecast(model8ts,newdata=newdata)
#Plotting the forecast vs original data
autoplot(Labor,main = "Model 8, Fitted Trend + Forecast:\nCivilian Labor Force Participation Rate",xlab='Year',ylab="Participation, (%)",col="gray") + autolayer(plot8,series="Trend") + autolayer(cast,series="Forecast") + scale_color_manual(values=c('blue','red'), breaks=c('Trend','Forecast'))
```

The figure above shows the forecast of the full model, with an $h=60$ point and interval forecasts. The grey series is the observed data. The lighter shade of blue corresponds to 95% forecast interval and the darker blue shade corresponds to 80% forecast interval.

### 2. Modeling and Forecasting Seasonality

**(a) Construct and test (by looking at the diagnostic statistics) a model with a full set of seasonal dummies. **

To estimate the seasonal effect, we use a set of 11 seasonal dummies to indicate whether a particular measurement occured in February, March, etc. It is important to isolate the trend component so the seasonality estimation does not become biased.
```{r}
# regress Labor on linear trend plus full seasonal dummies
seasonal=tslm(Labor~trend+season)
# print out summary of model
summary(seasonal)
```
From the test statistics, at the 5% level, the intercept, the linear trend, season2, season3, season6, and season7 are all statistically significant from zero. The other seasonal coefficients are not statistically significant from zero. The $R^2_{\mathrm{adj}}$ is 0.9682, indicating a fairly good fit for this simple model. The $p$-value for the $F$ test is lower than 5%, so we reject the null hypothesis that the there is no seasonality or trend at 5% level.

**(b) Plot the estimated seasonal factors and interpret your plot.**

The seasonal effect can be visualized by plotting all the seasonal coefficients. The seasonal coefficients indicate how much (on average) the labor force participation rate in a given month is different from that of January. Since our 11 seasonal dummies do not include January (otherwise there would be a collinearity issue and the regression would fail), we manually set the coefficient of January to be zero. 

```{r}
# extract seasonal coefs from model
seasfactors=seasonal$coefficients[2:13]
# manually set coef for January
names(seasfactors)[[1]]='season1' 
seasfactors[[1]]=0
# convert to ts object and autoplot
seasfactors %>% ts(start=1) %>% 
  autoplot(xlab='Month',ylab='Seasonal Factors, (% points)',
           main='Additive Seasonal Factors (January=0%):\nCivilian Labor Force Participation Rate')
```

There is a relative increase in civilian labor force participation rate in spring and fall with respect to January, and there is a relative decrease in civilian labor force participation rate in summer with respect to January. The suspected reason for the increase is that spring and fall are the major recruitment periods for firms.

**(c) In order to improve your model, add the trend model from problem 1 to your seasonal model. We will refer to this model as the full model. For the full model, plot the respective residuals vs. fitted values and discuss your observations.**

There are flaws in using a linear trend when we regresses for the seasonal factors: a linear trend just doesn't fit the data very well. Here we will use the quadratic and periodic trend which we selected from problem 1, together with the full set of seasonal dummies, to account for the variations in data. This will hopefully be an improvement to both the estimation of the seasonal factors and the estimation of a full model.

```{r}
# regress model on more sophisticated trend component and seasonal dummies
full=tslm(Labor~I(t*t)+t*cos(2*pi*t/13)+sin(2*pi*t/13)+season)
# plot the residuals versus the fitted
plot(full$fit,full$res,pch=20,cex=.6,col='skyblue3',
     xlab='Fitted values',ylab='Residuals',
     main='Residuals vs. Fitted Values for Full Model',
     ylim=c(-2,2))
# Plotting Y(Observed) = Y(Predicted) line
abline(h=0,lwd=1.5,col="red")
# plot the lowess smoother curve on top of the residuals
lines(lowess(full$fit,full$res),lwd=2)
# create a legend for the lowess smoother
legend(78,2, c(expression(y[obs]==y[pred]), "Lowess Smoother"), fill =c("red", "black"),cex=1,bty="y")
```

At all ranges of fitted values, the residuals look evenly and randomly distributed on both side of the zero line. The lowess smoother is nearly indistinguishable from the zero line. This means that our model did not miss any pattern in the data. The error looks like it has constant variance, which means we need not worry about heteroskedasticity in the regression process.

**(d) Interpret the respective summary statistics including the error metrics of your full model.**

To evaluate the full model, we look at the statistics from the summary.

```{r}
# print the summary, aic, and bic
summary(full)
AIC(full)
BIC(full)
```

Out of all the explanatory variables, only the seasonal dummies for season5, season9, season10, season11, season12 are not significant at the 5% level. This means that these seasons do not contribute much to explaining the seasonal variation of the data. The aic and bic values are lower than any of the previous models, meaning that including seasonal effects is crucial to explaining the variations of the data. The $R^2_{\mathrm{adj}}$ increased from the seasonal plus linear trend model, indicating that the trend from problem 1 fits the data better than a naive linear trend.

**(e) Use the full model to forecast h-steps (at least 16) ahead. Your forecast should include the respective prediction interval.**

Given the full model which seems to fit existing data well, we attempt to extract forecasts 60 periods ahead ($h=60$), in order to evaluate the degree to which the forecasted values for civilian labor force participation rate is probable.

```{r}
# construct new data to base forecast on
newt=seq(2020,2025,1/12)
newdata=data.frame(newt,cos(2*pi*newt/13),sin(2*pi*newt/13))
names(newdata)=c('t','cos(2 * pi * t/13)','sin(2 * pi * t/13)')
# embed forecast in plots
autoplot(Labor,col='grey',
         main='Full Model, Fitted + Forecast:\nCivilian Labor Force Participation Rate')+
  xlab('Year')+ylab('Participation, (%)')+
  autolayer(full$fitted.values,series='Fitted Values')+
  autolayer(forecast(full,newdata=newdata),series='Forecast')+
  scale_color_manual(values=c('red','blue'),
                     breaks=c('Fitted Values','Forecast'))
```

The figure above shows the forecast of the full model, with an $h=60$ point and interval forecasts. The grey series is the observed data. The lighter shade of blue corresponds to 95% forecast interval and the darker blue shade corresponds to 80% forecast interval. Notable features of the forecast: visually it seems to be a natrual extension to the data. The downward curve is a result of the estimated cycle. Seasonality is cleary present in the forecasts. The size of the prediction interval sees to be in agreement with the amount that actual data (grey) deviates from the fitted values.

We must note that the prediction interval which `forecast` produces must be taken with a grain of salt. This prediction interval does not take into account the error introduced when estimating the model parameters. Adding to that, the error in the cycle period is unaccountable given that we have dictated the period to be 13 based on trial and error. The true prediction interval should be larger.

## III. Conclusions and Future Work

**Conclusions**

To model the evolution of the civilian labor force participation rate (Bachelor's degree and higher, 25 years and over), our final chosen model is $$LABOR_t=\beta_0+\beta_1TIME_t+\beta_2TIME^2_t+(A_0+A_1TIME_t)\cos\left(\frac{2\pi}{13}TIME_t\right)+B_0\sin\left(\frac{2\pi}{13}TIME_t\right)+\sum_{m=2}^{12}\delta_mMONTH_{mt}$$
The first three terms in the model account for a quatratic trend. The next two terms account for a amplitude-varying 13 year cycle. The last term accounts for seasonality. 

The result of the full model regression is as follows:

1. The regression  produced a quadratic trend which concaves down, suggesting that the decrease in the labor force participation rate is accelerating. This could a result of many different interacting factors. First, it could be evidence of an aging population slowly entering into retirement age. Another potential cause might be a shift towards less labor intensive industries in the United States. Although decreasing the demand for labor primarily impacts those jobs that do not require a degree, we still might be seeing a bleedover of that effect into more advanced and technical industries. One specific example of such phenomena is that machine learning is beginning to take a more significant role in financial markets and such a shift could result in less demand for labor in these industries.
2. The data is modulated by a ~13 year cycle, whose amplitude is slowly increasing (on the scale of 0.01-0.02 percentage points per year). This is possibly related to the expansions and recessions of the business cycle. The dynamics of the cycle seems to destabilize as the quadratic trend rapidly accelerates downward. When at this behavior on a much larger forecast (such as 100 years), the labor force participation rate would thus tend to approach zero. This suggests that these 13 year cycles motivated by the performance of the economy as a whole will have little impact on the long run if the model serves to be true.
3. The seasonal part of the model suggests that the labor force participation rate is lower than average in summer and higher than average in spring and fall, which could be related to how businesses manages recruitment and separations logistically.

Based on these observations, the model forecasts that in the short run (5 years, $h=60$):

1. The decrease in labor force participation rate (coming from the quadratic component) will continue to speed up. Extrapolating onto a grander timescale, we see that the labor force participation rate will approach zero if this model is true. In all likelihood, this is not the case and we might see a different nature of behavior in the future. Or, if this is the case, it could be a result of automating away jobs at an acceralting rate, giving people less incentive to provide labor until the whole of economic production is largely being completed by automation and artificial intelligence for far cheaper than any laborer could offer.
2. The recession part of the business cycle will dominate the short run contraction of the labor force participation rate. If the model is true in this regard, it could potentially be a result of the advent of a recession which would discourage workers from searching for employment.
3. The seasonal variation will persist on top of the accelerating negative trend and fluctuating 13 month cycle.

**Future work:**

There are several points where our model could improve:

1. A lack of longtidutinal data: our model used civilian labor force participation rate from 1992 to 2019, some 28 years of data. If we could obtain the data from beyond 1992 for our demographic, we could verify whether the quadratic trend, the 13-year business cycle, and the roughly-constant seasonality are characteristics which persists in the long run, or they are themselves time-varying. With longer-term data, perhaps other trends and cycles are more appropriate.
2. A lack of latitudinal data: our model used data from the U.S. only, and we only included the civilian labor force participation rate of the people who have a Bachelor's degree or higher and are 25 years and over. If we used data from other subcategories of the population, and even other countries, we can verify whether or not the dynamics we see in our model are specific to the population and country. For example, when looking at data for the US population (and labor force participation) as a whole, we have seen that the negative trend in labor force participation generally only holds true after the turn of the century.
3. Seasonality assumed constant throughout the 28 years of data: This assumption is an artifact of the regression model we used. To verify whether this assumption is true and improve the model, it is advisable to perform seasonal decomposition with algorithms such as STL, X11, or SEATS.
4. Residuals assumed to be independent: we have not checked for serial correlation in the residuals of the full model. If there were serial correlation, an ARMA model would perhaps be appropriate.

## IV. References

[1] Civilian Labor Force Participation Rate: Bachelor's Degree and Higher, 25 years and over (LNU01327662), *FRED Economic Data*, accessed Jan 27, 2020, available at: https://fred.stlouisfed.org/series/LNU01327662

## V. R Source code

Provided above in text.

